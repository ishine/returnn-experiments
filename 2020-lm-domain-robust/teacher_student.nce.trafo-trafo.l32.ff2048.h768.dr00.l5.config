#!crnn/rnn.py
# kate: syntax python;
# multisetup: finished True; finished_reason 'unstable';


# ==========================================================
#   Knowledge Distillation
#   * 2 domain experts
#   * Trafo Teachers - Trafo Student
#   with Sampled Softmax and Noise Contrastive Estimation (default)
# ==========================================================


import os
from subprocess import check_output
from Util import cleanup_env_var_path
import sys
sys.path.insert(0, '/u/gerstenberger/returnn')

cleanup_env_var_path("LD_LIBRARY_PATH", "/u/zeyer/tools/glibc217")

import tensorflow as tf


# ==========================================================
# GENERAL
# ==========================================================

use_tensorflow  = True
device          = "gpu"
task            = "train"

# only at i6. At RWTH ITC we use the high performance filesystem which takes care of loading files.
use_cache_manager = False

tf_session_opts = {
  'allow_soft_placement': True,
  'log_device_placement': False}

DEBUG = config.bool('debug', False)

debug_print_layer_output_template   = True
debug_add_check_numerics_ops        = DEBUG
debug_add_check_numerics_on_output  = DEBUG

do_test_dataset     = config.bool('do-test', False)

num_epochs          = 90
train_partition     = 15
vocabulary_size     = 249352
seq_boundary        = '<sb>'
unk_token           = '<unk>'
replace_unk_auto    = True
seq_order           = 'sort_bin_shuffle:.{}'.format(config.int('max_seqs', 32))

domain_conditional  = False
domain_subset       = config.value('subset', 'background')  # which validation/test dataset to use 


# ==========================================================
# DATASETS
# ==========================================================

prefix_path = ''

data_files = {
  'train': ['{}/'.format(prefix_path),
            '{}/'.format(prefix_path)],
  'valid': ['{}/{}'.format(prefix_path, domain_subset)],
  'test': ['{}/{}'.format(prefix_path, domain_subset)]}
vocab_file = '{}/'.format(prefix_path)


# ==========================================================
# MODEL HYPERPARAMETER
# ==========================================================

teacher_loss_scale          = 0.5

use_nce                     = True
nce_bias_scale              = 1.5
num_sampled                 = 1024 if use_nce else 8192
full_softmax                = config.bool('full_softmax', False)  # just use student model with softmax for eval

num_layers                  = 32
num_heads                   = 16
ff_dim                      = 2048
emb_dim                     = 128
qk_dim                      = 768
v_dim                       = qk_dim
trans_out_dim               = qk_dim
dropout                     = 0.0
att_dropout                 = 0.0
act_func                    = "relu"

teacher_num_layers          = 32
teacher_num_heads           = 16
teacher_ff_dim              = 2048
teacher_emb_dim             = 128
teacher_qk_dim              = 768
teacher_v_dim               = teacher_qk_dim
teacher_trans_out_dim       = teacher_qk_dim
teacher_dropout             = 0.0
teacher_att_dropout         = 0.0

place_emb_on_cpu            = False  # E.g. for adagrad.
forward_weights_initializer = "variance_scaling_initializer(mode='fan_in', distribution='uniform', scale=1.0)"


# ==========================================================
# TRAINING HYPERPARAMETER
# ==========================================================

window                      = 1
cache_size                  = '0'

chunking                    = "0"
batching                    = 'random'
batch_size                  = 1024
max_seqs                    = 32
max_seq_length              = 512

gradient_clip_global_norm   = 1.0
gradient_noise              = 0.0

learning_rate                                       = 1.
newbob_learning_rate_decay                          = 0.95
newbob_relative_error_threshold                     = -0.01
newbob_relative_error_div_by_old                    = True
learning_rate_control                               = 'newbob_multi_epoch'
learning_rate_file                                  = "newbob.data"
learning_rate_control_relative_error_relative_lr    = True
learning_rate_control_error_measure                 = 'dev_score_output/output:exp'
newbob_multi_num_epochs                             = train_partition // 5
newbob_multi_update_interval                        = train_partition // 5

calculate_exp_loss = True
cleanup_old_models = {'keep_best_n': 1, 'keep_last_n': 1}

model           = "net-model/network"
log             = 'log/crnn.{}.log'.format(task)
log_verbosity   = 5 if DEBUG else 4


# ==========================================================
# COMPUTATION GRAPH
# ==========================================================

class Transformer:
    def __init__(self,
                 key,
                 num_heads,
                 num_layers,
                 ff_dim,
                 emb_dim,
                 qk_dim,
                 v_dim,
                 trans_out_dim,
                 dropout,
                 att_dropout,
                 act_func,
                 weights_init,
                 target='data',
                 final_layer_update=None,
                 trainable=True):
        """
        Defines a Transformer computation graph without loss function.

        :param key: name of transformer model
        :param num_heads:
        :param num_layers:
        :param ff_dim:
        :param emb_dim:
        :param qk_dim:
        :param v_dim:
        :param trans_out_dim:
        :param dropout:
        :param att_dropout:
        :param act_func:
        :param weights_init:
        :param target: if loss is set, target key
        :param final_layer_update: if not None, update the output layer's options, e.g. set a loss function.
        :param trainable: If False, freeze parameters.
        """
        self.output_name = key
        self.network = None
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.ff_dim = ff_dim
        self.emb_dim = emb_dim
        self.qk_dim = qk_dim
        self.v_dim = v_dim
        self.trans_out_dim = trans_out_dim
        self.dropout = dropout
        self.att_dropout = att_dropout
        self.act_func = act_func
        self.forward_weights_initializer = weights_init
        self.trainable = trainable
        self.tied_params = False
        self.target = target
        self.final_layer_update = final_layer_update
        self._initialize()

    def _initialize(self):
        self.network = {
            self.output_name: {
                'class': 'rec',
                'from': ['data:delayed'],
                'trainable': self.trainable,
                'unit': {'target_embed_raw': {'activation': None,
                                              "param_device": "CPU" if place_emb_on_cpu else None,
                                              'class': 'linear',
                                              'forward_weights_init': self.forward_weights_initializer,
                                              'from': ['data:source'],
                                              'n_out': self.emb_dim,
                                              'with_bias': False},
                         'target_embed': {'class': 'dropout', 'dropout': self.dropout, 'from': ['target_embed_raw']},
                         'target_embed_lin': {'activation': None,
                                              'class': 'linear',
                                              'forward_weights_init': self.forward_weights_initializer,
                                              'from': ['target_embed'],
                                              'n_out': self.trans_out_dim,
                                              'with_bias': False},
                         'dec_0': {'class': 'copy', 'from': ['dec_0_ff_out']},
                         'dec_0_self_att_laynorm': {'class': 'layer_norm', 'from': ['target_embed_lin']},
                         'dec_0_self_att_att': {'attention_dropout': self.att_dropout,
                                                'attention_left_only': True,
                                                'class': 'self_attention',
                                                'forward_weights_init': self.forward_weights_initializer,
                                                'from': ['dec_0_self_att_laynorm'],
                                                'n_out': self.v_dim,
                                                'num_heads': self.num_heads,
                                                'total_key_dim': self.qk_dim},
                         'dec_0_self_att_lin': {'activation': None,
                                                'class': 'linear',
                                                'forward_weights_init': self.forward_weights_initializer,
                                                'from': ['dec_0_self_att_att'],
                                                'n_out': self.trans_out_dim,
                                                'with_bias': False},
                         'dec_0_self_att_drop': {'class': 'dropout', 'dropout': self.dropout,
                                                 'from': ['dec_0_self_att_lin']},
                         'dec_0_att_out': {'class': 'combine',
                                           'from': ['target_embed_lin', 'dec_0_self_att_drop'],
                                           'kind': 'add',
                                           'n_out': self.trans_out_dim,
                                           'trainable': True},
                         'dec_0_ff_laynorm': {'class': 'layer_norm', 'from': ['dec_0_att_out']},
                         'dec_0_ff_conv1': {'activation': self.act_func,
                                            'class': 'linear',
                                            'forward_weights_init': self.forward_weights_initializer,
                                            'from': ['dec_0_ff_laynorm'],
                                            'n_out': self.ff_dim,
                                            'with_bias': True},
                         'dec_0_ff_conv2': {'activation': None,
                                            'class': 'linear',
                                            'dropout': self.dropout,
                                            'forward_weights_init': self.forward_weights_initializer,
                                            'from': ['dec_0_ff_conv1'],
                                            'n_out': self.trans_out_dim,
                                            'with_bias': True},
                         'dec_0_ff_drop': {'class': 'dropout', 'dropout': self.dropout, 'from': ['dec_0_ff_conv2']},
                         'dec_0_ff_out': {'class': 'combine', 'from': ['dec_0_att_out', 'dec_0_ff_drop'], 'kind': 'add',
                                          'n_out': self.trans_out_dim},}}}

    def _add_layer(self, cur_lay_id, prev_lay_id):
        self.network[self.output_name]['unit']['dec_%(cur_lay_id)s' % {'cur_lay_id': cur_lay_id}] = {
            'class': 'copy', 'from': ['dec_%(cur_lay_id)s_ff_out' % {'cur_lay_id': cur_lay_id}]}
        self.network[self.output_name]['unit']['dec_%(cur_lay_id)s_self_att_laynorm' % {'cur_lay_id': cur_lay_id}] = {
            'class': 'layer_norm', 'from': ['dec_%(prev_lay_id)s' % {'prev_lay_id': prev_lay_id}]}
        self.network[self.output_name]['unit']['dec_%(cur_lay_id)s_self_att_att' % {'cur_lay_id': cur_lay_id}] = {
            'attention_dropout': self.att_dropout,
            'attention_left_only': True,
            'reuse_params': 'dec_0_self_att_att' if self.tied_params else None,
            'class': 'self_attention',
            'forward_weights_init': self.forward_weights_initializer,
            'from': ['dec_%(cur_lay_id)s_self_att_laynorm' % {'cur_lay_id': cur_lay_id}],
            'n_out': self.v_dim,
            'num_heads': self.num_heads,
            'total_key_dim': self.qk_dim}
        self.network[self.output_name]['unit']['dec_%(cur_lay_id)s_self_att_lin' % {'cur_lay_id': cur_lay_id}] = {
            'activation': None,
            'class': 'linear',
            'reuse_params': 'dec_0_self_att_lin' if self.tied_params else None,
            'forward_weights_init': self.forward_weights_initializer,
            'from': ['dec_%(cur_lay_id)s_self_att_att' % {'cur_lay_id': cur_lay_id}],
            'n_out': self.trans_out_dim,
            'with_bias': False}
        self.network[self.output_name]['unit']['dec_%(cur_lay_id)s_self_att_drop' % {'cur_lay_id': cur_lay_id}] = {
            'class': 'dropout', 'dropout': self.dropout,
            'from': ['dec_%(cur_lay_id)s_self_att_lin' % {'cur_lay_id': cur_lay_id}]}
        self.network[self.output_name]['unit']['dec_%(cur_lay_id)s_att_out' % {'cur_lay_id': cur_lay_id}] = {
            'class': 'combine',
            'from': ['dec_%(prev_lay_id)s' % {'prev_lay_id': prev_lay_id},
                     'dec_%(cur_lay_id)s_self_att_drop' % {'cur_lay_id': cur_lay_id}],
            'kind': 'add',
            'n_out': self.trans_out_dim,
            'trainable': True}
        self.network[self.output_name]['unit']['dec_%(cur_lay_id)s_ff_laynorm' % {'cur_lay_id': cur_lay_id}] = {
            'class': 'layer_norm', 'from': ['dec_%(cur_lay_id)s_att_out' % {'cur_lay_id': cur_lay_id}]}
        self.network[self.output_name]['unit']['dec_%(cur_lay_id)s_ff_conv1' % {'cur_lay_id': cur_lay_id}] = {
            'class': 'linear',
            'activation': self.act_func,
            'forward_weights_init': self.forward_weights_initializer,
            'reuse_params': 'dec_0_ff_conv1' if self.tied_params else None,
            'from': ['dec_%(cur_lay_id)s_ff_laynorm' % {'cur_lay_id': cur_lay_id}],
            'n_out': self.ff_dim,
            'with_bias': True}
        self.network[self.output_name]['unit']['dec_%(cur_lay_id)s_ff_conv2' % {'cur_lay_id': cur_lay_id}] = {
            'class': 'linear',
            'activation': None,
            'dropout': self.dropout,
            'reuse_params': 'dec_0_ff_conv2' if self.tied_params else None,
            'forward_weights_init': self.forward_weights_initializer,
            'from': ['dec_%(cur_lay_id)s_ff_conv1' % {'cur_lay_id': cur_lay_id}],
            'n_out': self.trans_out_dim,
            'with_bias': True}
        self.network[self.output_name]['unit']['dec_%(cur_lay_id)s_ff_drop' % {'cur_lay_id': cur_lay_id}] = {
            'class': 'dropout', 'dropout': self.dropout,
            'from': ['dec_%(cur_lay_id)s_ff_conv2' % {'cur_lay_id': cur_lay_id}]}
        self.network[self.output_name]['unit']['dec_%(cur_lay_id)s_ff_out' % {'cur_lay_id': cur_lay_id}] = {
            'class': 'combine', 'from': ['dec_%(cur_lay_id)s_att_out' % {'cur_lay_id': cur_lay_id},
                                         'dec_%(cur_lay_id)s_ff_drop' % {'cur_lay_id': cur_lay_id}],
            'kind': 'add', 'n_out': self.trans_out_dim}

    def construct(self):
        # Stack layers.
        cur_lay_id = 1
        prev_lay_id = 0
        for i in range(self.num_layers - 1):
            self._add_layer(cur_lay_id, prev_lay_id)
            cur_lay_id += 1
            prev_lay_id += 1

        # Add the final layer norm.
        self.network[self.output_name]['unit']['decoder'] = {'class': 'layer_norm', 'from': ['dec_%s' % prev_lay_id]}

        self.network[self.output_name]['unit']['output'] = {
            'class': 'softmax',
            'dropout': self.dropout,
            'forward_weights_init': self.forward_weights_initializer,
            'from': ['decoder'],
            'target': self.target,
            'n_out': vocabulary_size,
            'with_bias': True}
        if self.final_layer_update is not None:
            self.network[self.output_name]['unit']['output'].update(self.final_layer_update)

        return self.network


class KDNetwork:
    def __init__(self,
                 network,
                 use_nce,
                 sampler_name,
                 number_sampled,
                 experts,
                 initializer,
                 teacher_ce_scale,
                 kd_domain_conditional,
                 nce_log_norm_term=0.0,
                 base_input="{}_output/decoder",
                 base_output_params="{}_output/output",
                 target="data"):
        self.network = network
        self.use_nce = use_nce
        self.num_sampled = number_sampled
        self.experts = experts
        self.sampler_name = sampler_name
        self.target = target
        self.nce_log_norm_term = nce_log_norm_term
        self.initializer = initializer
        self.base_input = base_input
        self.base_output_params = base_output_params
        self.teacher_loss_scale = teacher_ce_scale
        self.domain_conditional = kd_domain_conditional

    @staticmethod
    def sampling_loss(self, source):
        logits = source(0, auto_convert=False)  # [B'', 1 + num_sampled]
        labels = source(1, auto_convert=False)

        if use_nce:
            loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels,
                                                           logits=logits,
                                                           name="ce_loss")
            loss = tf.reduce_sum(loss, axis=1)
        else:
            labels = tf.stop_gradient(labels)
            loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels,
                                                              logits=logits,
                                                              name="ce_loss")

        return loss

    @staticmethod
    def make_targets(self, source):
        batch_size = tf.shape(source(0, auto_convert=False))[0]
        dim = num_sampled + 1

        # by construction, the target class is always at position 0
        oh = tf.one_hot(tf.zeros(shape=[batch_size], dtype=tf.int32), depth=dim)  # [B'', num_sampled + 1]

        return oh

    def _add_output_layers(self, expert):
        self.network['%s_logits' % expert.prefix] = {  # [B'', num_sampled + 1]
            'class': 'sampled_logits',
            'sampler': self.sampler_name,
            'labels': self.target,
            'nce': self.use_nce,
            'nce_log_norm_term': self.nce_log_norm_term,
            'forward_weights_init': self.initializer,
            'bias_init': self.initializer,
            'trainable': expert.trainable,
            'n_out': self.num_sampled + 1,
            'reuse_params': self.base_output_params.format(expert.prefix),
            'from': [self.base_input.format(expert.prefix)]}
        self.network['%s_out' % expert.prefix] = {
            'class': 'activation',
            'activation': 'sigmoid' if self.use_nce else 'softmax',
            'from': ['%s_logits' % expert.prefix]}

    def construct(self, student_output_name):
        for expert in self.experts:
            self._add_output_layers(expert)
        self.network['labels'] = {
            'class': 'eval',
            'eval': KDNetwork.make_targets,
            'from': ['soft_targets'],
            'out_type': {
                'dim': self.num_sampled + 1,
                'time_dim_axis': None,
                'batch_dim_axis': 0,
                'shape': (self.num_sampled + 1,)}}
        self.network['output_loss'] = {
            'class': 'eval',
            'eval': KDNetwork.sampling_loss,
            'is_output_layer': True,
            'loss': 'as_is',
            'loss_opts': {
                'scale': 1 - self.teacher_loss_scale},
            'from': [student_output_name, 'labels'],
            'out_type': {
                'shape': (),
                'time_dim_axis': None,
                'batch_dim_axis': 0,
                'feature_dim_axis': None,
                'dim': None}}
        self.network['kl_loss'] = {
            'class': 'eval',
            'eval': KDNetwork.sampling_loss,
            'is_output_layer': True,
            'loss': 'as_is',
            'loss_opts': {'scale': self.teacher_loss_scale},
            'from': [student_output_name, 'soft_targets'],
            'out_type': {
                'shape': (),
                'time_dim_axis': None,
                'batch_dim_axis': 0,
                'feature_dim_axis': None,
                'dim': None}}

        eval_str = []
        sources = []
        for idx, expert in enumerate(self.experts):
            eval_str += ['{} * source({})'.format(expert.weight, idx)]
            sources += ['%s_out' % expert.prefix]
        eval_str = ' + '.join(eval_str)
        self.network['soft_targets'] = {  # [B'', 1 + num_sampled]
            'class': 'eval',
            'eval': eval_str,
            'from': sources,
            'n_out': self.num_sampled + 1}

        if self.domain_conditional:
            assert len(self.experts) == 2, 'Only two experts supported currently'

            weights = []
            for x in [expert.weight for expert in self.experts]:
                weights.append([','.join(x)])
            weights = ','.join(weights)

            # TODO: adjust for more experts
            self.network['teacher_combined'] = {  # [B * T, num_sampled + 1, N]
                'class': 'eval',
                'eval': 'tf.stack([source(0, auto_convert=False), source(1, auto_convert=False)], axis=2)',
                'out_type': {
                    'shape': (self.num_sampled + 1, len(self.experts)),
                    'time_dim_axis': None,
                    'batch_dim_axis': 0,
                    'feature_dim_axis': 1,
                    'dim': self.num_sampled + 1},
                'from': sources}
            self.network['lambda_raw'] = {  # [B, T, N]
                'class': 'linear',
                'activation': None,
                'with_bias': False,
                'forward_weights_init': 'constant_initializer([%s])' % weights,
                'trainable': False,
                'from': ['data:source_id'],
                'n_out': len(self.experts)}
            self.network['lambda'] = {  # [B * T, N]
                'class': 'eval',
                'eval': 'flatten_with_seq_len_mask(source(0, auto_convert=False), source(1, as_data=True, auto_convert=False).get_sequence_lengths(), 0, 1)',
                'from': ['lambda_raw', 'data:delayed'],
                'out_type': {
                    'shape': (len(self.experts),),
                    'time_dim_axis': None,
                    'batch_dim_axis': 0}}
            self.network['teacher_scale'] = {  # [B * T, num_sampled + 1, N]
                'class': 'eval',
                'eval': 'source(0, auto_convert=False) * tf.expand_dims(source(1, auto_convert=False), axis=1)',
                'from': ['teacher_combined', 'lambda'],
                'out_type': {
                    'shape': (self.num_sampled + 1, len(self.experts)),
                    'time_dim_axis': None,
                    'batch_dim_axis': 0,
                    'feature_dim_axis': 1,
                    'dim': self.num_sampled + 1}}
            self.network['soft_targets'] = {  # [B * T, num_sampled + 1]
                'class': 'reduce',
                'mode': 'sum',
                'axes': 2,
                'keep_dims': False,
                'from': ['teacher_scale']}

        for expert in self.experts:
            self.network.update(expert.network)

        return self.network


class Expert:
    def __init__(self,
                 prefix,
                 network,
                 weight,
                 load_checkpoint,
                 trainable=False):
        self._prefix = prefix
        self._network = network
        self._weight = weight
        self._load_checkpoint = load_checkpoint
        self._trainable = trainable

    @property
    def prefix(self):
        return self._prefix

    @property
    def network(self):
        return self._network

    @property
    def weight(self):
        return self._weight

    @property
    def checkpoint_filename(self):
        return self._load_checkpoint

    @property
    def trainable(self):
        return self._trainable

    def preload_dict(self):
        return {
            self.prefix: {
                'filename': self.checkpoint_filename,
                'prefix': '{}_'.format(self.prefix),
                'init_for_train': True}}

def get_bias_init(scale):
  import math

  init = scale * math.log(1 / vocabulary_size)
  return 'constant_initializer(value={})'.format(init)

if use_nce:
  b_init = get_bias_init(nce_bias_scale)
else:
  b_init = forward_weights_initializer

network = {}

teacher_news = Transformer(key='teacher_news_output',
                           num_heads=teacher_num_heads,
                           num_layers=teacher_num_layers,
                           ff_dim=teacher_ff_dim,
                           emb_dim=teacher_emb_dim,
                           qk_dim=teacher_qk_dim,
                           v_dim=teacher_v_dim,
                           trans_out_dim=teacher_trans_out_dim,
                           dropout=teacher_dropout,
                           att_dropout=teacher_att_dropout,
                           act_func=act_func,
                           weights_init=forward_weights_initializer,
                           target=None,
                           final_layer_update={
                             'use_transposed_weights': True},
                           trainable=False)
teacher_news.construct()

teacher_movies = Transformer(key='teacher_movies_output',
                             num_heads=teacher_num_heads,
                             num_layers=teacher_num_layers,
                             ff_dim=teacher_ff_dim,
                             emb_dim=teacher_emb_dim,
                             qk_dim=teacher_qk_dim,
                             v_dim=teacher_v_dim,
                             trans_out_dim=teacher_trans_out_dim,
                             dropout=teacher_dropout,
                             att_dropout=teacher_att_dropout,
                             act_func=act_func,
                             weights_init=forward_weights_initializer,
                             target=None,
                             final_layer_update={
                               'use_transposed_weights': True},
                             trainable=False)
teacher_movies.construct()

student = Transformer(key='output',
                      num_heads=num_heads,
                      num_layers=num_layers,
                      ff_dim=ff_dim,
                      emb_dim=emb_dim,
                      qk_dim=qk_dim,
                      v_dim=v_dim,
                      trans_out_dim=trans_out_dim,
                      dropout=dropout,
                      att_dropout=dropout,
                      act_func=act_func,
                      weights_init=forward_weights_initializer,
                      target='data',
                      final_layer_update={
                          'bias_init': b_init,
                          'use_transposed_weights': True,
                          'loss': 'ce' if full_softmax else None,
                          'is_output_layer': full_softmax},
                      trainable=True)
network.update(student.construct())

if not full_softmax:
    # TODO: reconsider this setup. I don't like it very much.
    experts = [Expert(prefix='teacher_news',
                      network=teacher_news.network,
                      weight=0.516623 if not domain_conditional else [0.864935, 0.233055],
                      load_checkpoint='',
                      trainable=teacher_news.trainable),
               Expert(prefix='teacher_movies',
                      network=teacher_movies.network,
                      weight=0.483377 if not domain_conditional else [0.135065, 0.766945],
                      load_checkpoint='',
                      trainable=teacher_movies.trainable)]

    preload_from_files = {}
    for expert in experts:
        preload_from_files.update(expert.preload_dict())

    kd_network = KDNetwork(network=network,
                           use_nce=use_nce,
                           sampler_name='sampler',
                           number_sampled=num_sampled,
                           experts=experts,
                           initializer=forward_weights_initializer,
                           teacher_ce_scale=teacher_loss_scale,
                           kd_domain_conditional=domain_conditional,
                           nce_log_norm_term=0.0)

    # reuse params from output/output so we can easily remove this layer from the graph later.
    network['output_sampled'] = {  # [B'', num_sampled + 1]
        'class': 'sampled_logits',
        'sampler': kd_network.sampler_name,
        'labels': kd_network.target,
        'nce': kd_network.use_nce,
        'nce_log_norm_term': kd_network.nce_log_norm_term,
        'forward_weights_init': kd_network.initializer,
        'bias_init': b_init,
        'reuse_params': 'output/output',
        'is_output_layer': False,
        'n_out': num_sampled + 1,
        'from': ['output/decoder']}
    network['sampler'] = {
        'class': 'sampling',
        'sampler': 'log_uniform',
        'num_sampled': num_sampled,
        'from': ['data']}

    kd_network.construct('output_sampled')


# ==========================================================
# DETAILS
# ==========================================================

num_inputs = vocabulary_size

extern_data = {"data": {"dim": num_inputs, "sparse": True, "dtype": "int32"}}  # sparse data
extern_data["delayed"] = extern_data["data"]
target = "data"

_cf_cache = {}


def cf(filename):
    """Cache manager"""
    if not use_cache_manager:
        return filename
    if filename in _cf_cache:
        return _cf_cache[filename]
    if check_output(["hostname"]).strip() in ["cluster-cn-211", "sulfid"]:
        print("use local file: %s" % filename)
        return filename  # for debugging
    cached_fn = check_output(["cf", filename]).strip().decode("utf8")
    assert os.path.exists(cached_fn)
    _cf_cache[filename] = cached_fn
    return cached_fn


def get_dataset(key, **kwargs):
    assert key in ['train', 'valid', 'test']

    up = {
        'class': 'LmDataset',
        'corpus_file': lambda: list(map(cf, data_files[key])),
        'orth_symbols_map_file': lambda: cf(vocab_file),
        'word_based': True,
        'seq_end_symbol': seq_boundary,
        'auto_replace_unknown_symbol': replace_unk_auto,
        'unknown_symbol': unk_token,
        'add_delayed_seq_data': True,
        'delayed_seq_data_start_symbol': seq_boundary,
        'seq_ordering': 'sorted',
        'partition_epoch': 1,
        'parse_orth_opts': {
            'square_brackets_for_specials': False}}
    up.update(kwargs)

    return up


train = get_dataset("train", partition_epoch=train_partition, seq_ordering=seq_order)
dev = get_dataset("valid")
eval = get_dataset('test') if do_test_dataset else None


# ==========================================================
# IMPLEMENTATION
# ==========================================================

from TFNetworkLayer import register_layer_class, LayerBase, _ConcatInputLayer


class SamplingLayer(_ConcatInputLayer):
    layer_class = "sampling"

    def __init__(self, num_sampled, sampler='log_uniform', sampler_args=None, **kwargs):
        super(SamplingLayer, self).__init__(**kwargs)

        from tensorflow.python.ops import candidate_sampling_ops
        sampler_dict = {
            "uniform": candidate_sampling_ops.uniform_candidate_sampler,
            "log_uniform": candidate_sampling_ops.log_uniform_candidate_sampler,
            "learned_unigram": candidate_sampling_ops.learned_unigram_candidate_sampler,
            "fixed_unigram": candidate_sampling_ops.fixed_unigram_candidate_sampler,
            "all": candidate_sampling_ops.all_candidate_sampler}  # only for debugging purpose

        assert sampler in sampler_dict, "Sampler must be one of 'uniform', 'log_uniform', 'learned_unigram' or 'fixed_unigram', 'all'."

        if sampler_args is None:
            sampler_args = {}

        if sampler != "all":
            sampler_args.update({'range_max': self.input_data.dim})

        self.sampler_args = sampler_args
        self.num_sampled = num_sampled
        self.sampler = sampler_dict[sampler]

        with tf.name_scope("sampling"):
            from TFUtil import flatten_with_seq_len_mask
            labels = tf.reshape(flatten_with_seq_len_mask(self.input_data.placeholder,
                                                          self.input_data.get_sequence_lengths(),
                                                          time_major=self.input_data.is_time_major),
                                [-1, 1])

            if labels.dtype != tf.int64:
                labels = tf.cast(labels, dtype=tf.int64)

            # TODO: use a subnetwork to return the tuple
            # flatten the tuple into one tensor to be set as placeholder.
            sampled_candidates, true_expected_count, sampled_expected_count = \
                self.sampler(true_classes=labels,
                             num_true=1,
                             num_sampled=self.num_sampled,
                             unique=True,
                             **self.sampler_args)
            x = tf.concat([tf.cast(tf.reshape(sampled_candidates, [-1]), tf.float32),
                           tf.reshape(true_expected_count, [-1]),
                           tf.reshape(sampled_expected_count, [-1])],
                          axis=0)

            self.output.placeholder = tf.expand_dims(x, axis=0)  # [B = 1, 2 * num_sampled + B'']

    @classmethod
    def get_out_data_from_opts(cls, name, **kwargs):
        from TFUtil import Data as Data

        # TODO: maybe also adjust the size_placeholder
        return Data(
            name='%s_sampled' % name,
            shape=(None,),
            batch_dim_axis=0,
            time_dim_axis=1,
            sparse=False)


class SampledLogitsLayer(_ConcatInputLayer):
    """
    Layer returns the logits for sampled softmax / Noise contrastive estimation.
    """

    layer_class = 'sampled_logits'

    def __init__(self, sampler, labels, nce=False, nce_log_norm_term=0.0,
                 forward_weights_init="glorot_uniform", bias_init="0",
                 **kwargs):
        super(SampledLogitsLayer, self).__init__(**kwargs)

        self.labels = labels.output  # targets

        assert isinstance(sampler, SamplingLayer)
        self.sampler = sampler
        self.num_sampled = self.sampler.num_sampled
        self.nce = nce
        self.nce_log_norm_term = nce_log_norm_term
        self.num_classes = self.labels.dim

        with self.var_creation_scope():
            from TFUtil import get_initializer
            fwd_weights_initializer = get_initializer(forward_weights_init,
                                                      seed=self.network.random.randint(2 ** 31),
                                                      eval_local_ns={"layer": self})
            bias_weights_initializer = get_initializer(bias_init,
                                                       seed=self.network.random.randint(2 ** 31) if bias_init else 0,
                                                       eval_local_ns={"layer": self})

            self.softmax_weights = self.add_param(tf.get_variable(name="W",
                                                                  shape=(self.num_classes, self.input_data.dim),
                                                                  dtype=tf.float32,
                                                                  initializer=fwd_weights_initializer))
            self.softmax_bias = self.add_param(tf.get_variable(name="b",
                                                               shape=(self.num_classes,),
                                                               dtype=tf.float32,
                                                               initializer=bias_weights_initializer))

        with tf.name_scope("sampled_logits"):
            from TFUtil import flatten_with_seq_len_mask
            flat_labels = tf.reshape(flatten_with_seq_len_mask(self.labels.placeholder,
                                                               self.labels.get_sequence_lengths(),
                                                               time_major=self.labels.is_time_major),
                                     [-1, 1])  # [B'', 1]
            flat_input = flatten_with_seq_len_mask(self.input_data.placeholder,
                                                   self.input_data.get_sequence_lengths(),
                                                   time_major=self.input_data.is_time_major)  # [B'', D]

            if flat_labels.dtype != tf.int64:
                flat_labels = tf.cast(flat_labels, dtype=tf.int64)

            # TODO: remove this and do the splitting in the network instead
            # reconstruct tupled flattened in the sampling layer to pass to compute_sampled_logits
            sampled = tf.squeeze(self.sampler.output.placeholder, axis=0)
            sampled_candidates = tf.cast(tf.reshape(sampled[:self.num_sampled],
                                                    [self.num_sampled]),
                                         tf.int64)  # [num_sampled]
            true_expected_count = tf.reshape(sampled[self.num_sampled:-self.num_sampled],
                                             [-1, 1])  # [B'', 1]
            sampled_expected_count = tf.reshape(sampled[-self.num_sampled:],
                                                [self.num_sampled])  # [num_sampled]
            sampled_values = (sampled_candidates, true_expected_count, sampled_expected_count)

            from TFUtil import compute_sampled_logits
            output_logits, _ = \
                compute_sampled_logits(weights=self.softmax_weights,
                                       biases=self.softmax_bias,
                                       labels=flat_labels,
                                       inputs=flat_input,
                                       num_sampled=self.num_sampled,
                                       num_classes=self.num_classes,
                                       num_true=1,
                                       sampled_values=sampled_values,
                                       subtract_log_q=True,
                                       remove_accidental_hits=not self.nce,
                                       partition_strategy="div",
                                       name="sampled_logits",
                                       seed=None)
            if self.nce:
                output_logits -= self.nce_log_norm_term
            self.output.placeholder = output_logits  # [B'', num_sampled + 1]

    @classmethod
    def transform_config_dict(cls, d, network, get_layer):
        super(SampledLogitsLayer, cls).transform_config_dict(d, network=network, get_layer=get_layer)

        d["sampler"] = get_layer(d["sampler"])
        d["labels"] = get_layer(d["labels"])

    @classmethod
    def get_out_data_from_opts(cls, name, sampler, **kwargs):
        from TFUtil import Data as Data

        return Data(
            name='%s_sampled_logits' % name,
            shape=(sampler.num_sampled + 1,),  # [B'', D]
            batch_dim_axis=0,
            time_dim_axis=None,
            sparse=False)


register_layer_class(SampledLogitsLayer)
register_layer_class(SamplingLayer)
