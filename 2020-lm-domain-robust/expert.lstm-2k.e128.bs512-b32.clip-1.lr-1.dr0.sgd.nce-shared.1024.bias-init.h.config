#!crnn/rnn.py
# kate: syntax python;

# ==========================================================
# Example config file for a single LSTM domain expert using NCE loss.
# ==========================================================


import os
from subprocess import check_output
from Util import cleanup_env_var_path
import sys
sys.path.insert(0, '/u/gerstenberger/returnn')

cleanup_env_var_path("LD_LIBRARY_PATH", "/u/zeyer/tools/glibc217")

import tensorflow as tf


# ==========================================================
# GENERAL
# ==========================================================

use_tensorflow  = True
device          = "gpu"
task            = "train"

# TODO: only at i6. At RWTH ITC we use the high performance filesystem which takes care of loading files.
use_cache_manager = False

tf_session_opts = {'allow_soft_placement': True, 'log_device_placement': False}

DEBUG = config.bool('debug', False)

tf_log_memory_usage                 = True
debug_print_layer_output_template   = True
debug_add_check_numerics_ops        = DEBUG
debug_add_check_numerics_on_output  = DEBUG
flat_net_construction               = True

num_epochs          = 60
train_partition     = 10
vocabulary_size     = 249352
seq_boundary        = '<sb>'
unk_token           = '<unk>'
replace_unk_auto    = False
seq_order           = 'sort_bin_shuffle:.{}'.format(config.int('max_seqs', 32))

domain_subset       = config.value('subset', 'background')


# ==========================================================
# DATASETS
# ==========================================================

# TODO: set paths for your training/dev/eval data and vocabulary

do_test_dataset = False

prefix          = ''
prefix_train    = ''

data_files = {
  'train': ['{}/'.format(prefix_train)],
  'valid': ['{}/{}'.format(prefix, domain_subset)],
  'test': ['{}/{}'.format(prefix, domain_subset)]}
vocab_file = '{}/'.format(prefix)

# ==========================================================
# MODEL HYPERPARAMETER
# ==========================================================

use_nce                     = True  # sampled softmax otherwise
nce_bias_scale              = 1.5
num_sampled                 = 1024 if use_nce else 8192
full_softmax                = config.bool('full_softmax', False)  # just use student model with softmax for eval

num_layers                  = 2
lstm_dim                    = 2048
emb_dim                     = 128
dropout                     = 0.0

place_emb_on_cpu            = False  # E.g. for adagrad.
forward_weights_initializer = 'random_normal_initializer(mean=0.0, stddev=0.1)'


def get_bias_init(scale):
  import math

  init = scale * math.log(1 / vocabulary_size)
  return 'constant_initializer(value={})'.format(init)

if use_nce:
  b_init = get_bias_init(nce_bias_scale)
else:
  b_init = forward_weights_initializer


# ==========================================================
# TRAINING HYPERPARAMETER
# ==========================================================

window                      = 1
cache_size                  = '0'

chunking                    = "0"
batching                    = 'random'
batch_size                  = 1024
max_seqs                    = 32
max_seq_length              = 512

gradient_clip_global_norm   = 1.0
gradient_noise              = 0.0

learning_rate                                       = 1.0
newbob_learning_rate_decay                          = 0.9
newbob_relative_error_threshold                     = -0.01
newbob_relative_error_div_by_old                    = True
learning_rate_control                               = 'newbob_rel'
learning_rate_file                                  = "newbob.data"
learning_rate_control_relative_error_relative_lr    = True
learning_rate_control_error_measure                 = 'dev_score_output:exp'
newbob_multi_num_epochs                             = train_partition // 5
newbob_multi_update_interval                        = train_partition // 5

calculate_exp_loss = True
cleanup_old_models = {'keep_best_n': 1, 'keep_last_n': 1}

model           = "net-model/network"
log             = 'log/crnn.{}.log'.format(task)
log_verbosity   = 5 if DEBUG else 4


# ==========================================================
# COMPUTATION GRAPH
# ==========================================================

class LSTM:
    def __init__(self,
                 key,
                 num_layers,
                 hidden_dim,
                 emb_dim,
                 dropout,
                 weights_init,
                 target='data',
                 final_layer_update=None,
                 trainable=True):
        self.output_name = key
        self.network = None
        self.num_layers = num_layers
        self.emb_dim = emb_dim
        self.hidden_dim = hidden_dim
        self.dropout = dropout
        self.forward_weights_initializer = weights_init
        self.trainable = trainable
        self.target = target
        self.final_layer_update = final_layer_update
        self._initialize()

    def _get_layer_name(self, name):
        return '%s_%s' % (self.output_name, name) if self.output_name is not None else name

    def _initialize(self):
        self.network = {}
        self.network[self._get_layer_name('input')] = {
            'class': 'linear',
            'activation': None,
            'forward_weights_init': self.forward_weights_initializer,
            "with_bias": False,
            'trainable': self.trainable,
            'n_out': self.emb_dim,
            'from': ['data:delayed']}
        self.network[self._get_layer_name('output')] = {
            'class': 'softmax',
            'dropout': self.dropout,
            'forward_weights_init': self.forward_weights_initializer,
            'bias_init': self.forward_weights_initializer,
            'from': [self._get_layer_name('lstm{}'.format(self.num_layers - 1))],
            'target': self.target,
            'n_out': vocabulary_size,
            'trainable': self.trainable,
            'with_bias': True}
        if self.final_layer_update is not None:
            self.network[self._get_layer_name('output')].update(self.final_layer_update)

    def construct(self):
        self.network[self._get_layer_name('lstm0')] = {
            'class': 'rec',
            'unit': 'nativelstm2',
            'direction': 1,
            'forward_weights_init': self.forward_weights_initializer,
            'recurrent_weights_init': self.forward_weights_initializer,
            'bias_init': self.forward_weights_initializer,
            'dropout': self.dropout,
            'n_out': self.hidden_dim,
            'trainable': self.trainable,
            'from': [self._get_layer_name('input')]}
        prev = 'lstm0'
        for i in range(1, self.num_layers):
            curr = 'lstm{}'.format(i)
            self.network[self._get_layer_name(curr)] = {
                'class': 'rec',
                'unit': 'nativelstm2',
                'direction': 1,
                'forward_weights_init': self.forward_weights_initializer,
                'recurrent_weights_init': self.forward_weights_initializer,
                'bias_init': self.forward_weights_initializer,
                'dropout': self.dropout,
                'trainable': self.trainable,
                'n_out': self.hidden_dim,
                'from': [self._get_layer_name(prev)]}
            prev = curr
        return self.network


network = {}
student = LSTM(key=None,  # set no prefix. Used for knowledge distillation.
               num_layers=num_layers,
               hidden_dim=lstm_dim,
               emb_dim=emb_dim,
               dropout=dropout,
               weights_init=forward_weights_initializer,
               target='data',
               final_layer_update={
                 'use_transposed_weights': True,
                 'loss': 'sampling_loss',
                 'loss_opts': {
                    'num_sampled': num_sampled,
                    'sampler': 'log_uniform',
                    'use_full_softmax': full_softmax,
                    'nce_log_norm_term': 0.0,
                    'nce_loss': use_nce},
                 'bias_init': b_init},
               trainable=True)
network.update(student.construct())


# ==========================================================
# DETAILS
# ==========================================================

num_inputs = vocabulary_size

extern_data = {"data": {"dim": num_inputs, "sparse": True, "dtype": "int32"}}  # sparse data
extern_data["delayed"] = extern_data["data"]
target = "data"

_cf_cache = {}


def cf(filename):
    """Cache manager"""
    if not use_cache_manager:
        return filename
    if filename in _cf_cache:
        return _cf_cache[filename]
    if check_output(["hostname"]).strip() in ["cluster-cn-211", "sulfid"]:
        print("use local file: %s" % filename)
        return filename  # for debugging
    cached_fn = check_output(["cf", filename]).strip().decode("utf8")
    assert os.path.exists(cached_fn)
    _cf_cache[filename] = cached_fn
    return cached_fn


def get_dataset(key, **kwargs):
    assert key in ['train', 'valid', 'test']

    up = {
        'class': 'LmDataset',
        'corpus_file': lambda: list(map(cf, data_files[key])),
        'orth_symbols_map_file': lambda: cf(vocab_file),
        'word_based': True,
        'seq_end_symbol': seq_boundary,
        'auto_replace_unknown_symbol': replace_unk_auto,
        'unknown_symbol': unk_token,
        'add_delayed_seq_data': True,
        'delayed_seq_data_start_symbol': seq_boundary,
        'seq_ordering': 'sorted',
        'partition_epoch': 1,
        'parse_orth_opts': {
            'square_brackets_for_specials': False}}
    up.update(kwargs)

    return up


train = get_dataset("train",
                    partition_epoch=train_partition,
                    seq_ordering=seq_order)
dev = get_dataset("valid")
eval = get_dataset('test') if do_test_dataset else None

