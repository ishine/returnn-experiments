#!crnn/rnn.py
# kate: syntax python;

# ==========================================================
#   Example config file for a single Transformer domain expert using NCE loss.
# ==========================================================


import os
from subprocess import check_output
from Util import cleanup_env_var_path
import sys
sys.path.insert(0, '/u/gerstenberger/returnn')

cleanup_env_var_path("LD_LIBRARY_PATH", "/u/zeyer/tools/glibc217")

import tensorflow as tf


# ==========================================================
# GENERAL
# ==========================================================

use_tensorflow  = True
device          = "gpu"
task            = "train"

# TODO: only at i6. At RWTH ITC we use the high performance filesystem which takes care of loading files.
use_cache_manager = False

tf_session_opts = {'allow_soft_placement': True, 'log_device_placement': False}

DEBUG = config.bool('debug', False)

tf_log_memory_usage                 = True
debug_print_layer_output_template   = True
debug_add_check_numerics_ops        = DEBUG
debug_add_check_numerics_on_output  = DEBUG
flat_net_construction               = True


num_epochs          = 60
train_partition     = 10
vocabulary_size     = 249352
seq_boundary        = '<sb>'
unk_token           = '<unk>'
replace_unk_auto    = False
seq_order           = 'sort_bin_shuffle:.{}'.format(config.int('max_seqs', 32))

domain_subset       = config.value('subset', 'background')


# ==========================================================
# DATASETS
# ==========================================================

# TODO: set paths for your training/dev/eval data and vocabulary

do_test_dataset = False

prefix          = ''
prefix_train    = ''

data_files = {
  'train': ['{}/'.format(prefix_train)],
	'valid': ['{}/{}'.format(prefix, domain_subset)],
	'test': ['{}/{}'.format(prefix, domain_subset)]}
vocab_file = '{}/'.format(prefix)

# ==========================================================
# MODEL HYPERPARAMETER
# ==========================================================

use_nce                     = True
nce_bias_scale              = 1.5
num_sampled                 = 1024 if use_nce else 8192
full_softmax                = config.bool('full_softmax', False)  # just use student model with softmax for eval

num_layers                  = 32
num_heads                   = 16
ff_dim                      = 2048
emb_dim                     = 128
qk_dim                      = 768
v_dim                       = qk_dim
trans_out_dim               = qk_dim
dropout                     = 0.0
att_dropout                 = 0.0
act_func                    = "relu"

place_emb_on_cpu            = False  # E.g. for adagrad.
forward_weights_initializer = "variance_scaling_initializer(mode='fan_in', distribution='uniform', scale=1.0)"


# ==========================================================
# TRAINING HYPERPARAMETER
# ==========================================================

window                      = 1
cache_size                  = '0'

chunking                    = "0"
batching                    = 'random'
batch_size                  = 1024
max_seqs                    = 32
max_seq_length              = 512

gradient_clip_global_norm   = 1.0
gradient_noise              = 0.0

learning_rate                                       = 1.
newbob_learning_rate_decay                          = 0.95
newbob_relative_error_threshold                     = -0.01
newbob_relative_error_div_by_old                    = True
learning_rate_control                               = 'newbob_rel'
learning_rate_file                                  = "newbob.data"
learning_rate_control_relative_error_relative_lr    = True
learning_rate_control_error_measure                 = 'dev_score_output/output:exp'
newbob_multi_num_epochs                             = train_partition // 5
newbob_multi_update_interval                        = train_partition // 5

calculate_exp_loss = True
cleanup_old_models = {'keep_best_n': 1, 'keep_last_n': 1}

model           = "net-model/network"
log             = 'log/crnn.{}.log'.format(task)
log_verbosity   = 5 if DEBUG else 4


def get_bias_init(scale):
  import math

  init = scale * math.log(1 / vocabulary_size)
  return 'constant_initializer(value={})'.format(init)

if use_nce:
  b_init = get_bias_init(nce_bias_scale)
else:
  b_init = forward_weights_initializer


# ==========================================================
# COMPUTATION GRAPH
# ==========================================================

class Transformer:
    def __init__(self,
                 key,
                 num_heads,
                 num_layers,
                 ff_dim,
                 emb_dim,
                 qk_dim,
                 v_dim,
                 trans_out_dim,
                 dropout,
                 att_dropout,
                 act_func,
                 weights_init,
                 target='data',
                 final_layer_update=None,
                 trainable=True):
        """
        Defines a Transformer computation graph without loss function.

        :param key: name of transformer model
        :param num_heads:
        :param num_layers:
        :param ff_dim:
        :param emb_dim:
        :param qk_dim:
        :param v_dim:
        :param trans_out_dim:
        :param dropout:
        :param att_dropout:
        :param act_func:
        :param weights_init:
        :param target: if loss is set, target key
        :param final_layer_update: if not None, update the output layer's options, e.g. set a loss function.
        :param trainable: If False, freeze parameters.
        """
        self.output_name = key
        self.network = None
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.ff_dim = ff_dim
        self.emb_dim = emb_dim
        self.qk_dim = qk_dim
        self.v_dim = v_dim
        self.trans_out_dim = trans_out_dim
        self.dropout = dropout
        self.att_dropout = att_dropout
        self.act_func = act_func
        self.forward_weights_initializer = weights_init
        self.trainable = trainable
        self.tied_params = False
        self.target = target
        self.final_layer_update = final_layer_update
        self._initialize()

    def _initialize(self):
        self.network = {
            self.output_name: {
                'class': 'rec',
                'from': ['data:delayed'],
                'trainable': self.trainable,
                'unit': {'target_embed_raw': {'activation': None,
                                              "param_device": "CPU" if place_emb_on_cpu else None,
                                              'class': 'linear',
                                              'forward_weights_init': self.forward_weights_initializer,
                                              'from': ['data:source'],
                                              'n_out': self.emb_dim,
                                              'with_bias': False},
                         'target_embed': {'class': 'dropout', 'dropout': self.dropout, 'from': ['target_embed_raw']},
                         'target_embed_lin': {'activation': None,
                                              'class': 'linear',
                                              'forward_weights_init': self.forward_weights_initializer,
                                              'from': ['target_embed'],
                                              'n_out': self.trans_out_dim,
                                              'with_bias': False},
                         'dec_0': {'class': 'copy', 'from': ['dec_0_ff_out']},
                         'dec_0_self_att_laynorm': {'class': 'layer_norm', 'from': ['target_embed_lin']},
                         'dec_0_self_att_att': {'attention_dropout': self.att_dropout,
                                                'attention_left_only': True,
                                                'class': 'self_attention',
                                                'forward_weights_init': self.forward_weights_initializer,
                                                'from': ['dec_0_self_att_laynorm'],
                                                'n_out': self.v_dim,
                                                'num_heads': self.num_heads,
                                                'total_key_dim': self.qk_dim},
                         'dec_0_self_att_lin': {'activation': None,
                                                'class': 'linear',
                                                'forward_weights_init': self.forward_weights_initializer,
                                                'from': ['dec_0_self_att_att'],
                                                'n_out': self.trans_out_dim,
                                                'with_bias': False},
                         'dec_0_self_att_drop': {'class': 'dropout', 'dropout': self.dropout,
                                                 'from': ['dec_0_self_att_lin']},
                         'dec_0_att_out': {'class': 'combine',
                                           'from': ['target_embed_lin', 'dec_0_self_att_drop'],
                                           'kind': 'add',
                                           'n_out': self.trans_out_dim,
                                           'trainable': True},
                         'dec_0_ff_laynorm': {'class': 'layer_norm', 'from': ['dec_0_att_out']},
                         'dec_0_ff_conv1': {'activation': self.act_func,
                                            'class': 'linear',
                                            'forward_weights_init': self.forward_weights_initializer,
                                            'from': ['dec_0_ff_laynorm'],
                                            'n_out': self.ff_dim,
                                            'with_bias': True},
                         'dec_0_ff_conv2': {'activation': None,
                                            'class': 'linear',
                                            'dropout': self.dropout,
                                            'forward_weights_init': self.forward_weights_initializer,
                                            'from': ['dec_0_ff_conv1'],
                                            'n_out': self.trans_out_dim,
                                            'with_bias': True},
                         'dec_0_ff_drop': {'class': 'dropout', 'dropout': self.dropout, 'from': ['dec_0_ff_conv2']},
                         'dec_0_ff_out': {'class': 'combine', 'from': ['dec_0_att_out', 'dec_0_ff_drop'], 'kind': 'add',
                                          'n_out': self.trans_out_dim},}}}

    def _add_layer(self, cur_lay_id, prev_lay_id):
        self.network[self.output_name]['unit']['dec_%(cur_lay_id)s' % {'cur_lay_id': cur_lay_id}] = {
            'class': 'copy', 'from': ['dec_%(cur_lay_id)s_ff_out' % {'cur_lay_id': cur_lay_id}]}
        self.network[self.output_name]['unit']['dec_%(cur_lay_id)s_self_att_laynorm' % {'cur_lay_id': cur_lay_id}] = {
            'class': 'layer_norm', 'from': ['dec_%(prev_lay_id)s' % {'prev_lay_id': prev_lay_id}]}
        self.network[self.output_name]['unit']['dec_%(cur_lay_id)s_self_att_att' % {'cur_lay_id': cur_lay_id}] = {
            'attention_dropout': self.att_dropout,
            'attention_left_only': True,
            'reuse_params': 'dec_0_self_att_att' if self.tied_params else None,
            'class': 'self_attention',
            'forward_weights_init': self.forward_weights_initializer,
            'from': ['dec_%(cur_lay_id)s_self_att_laynorm' % {'cur_lay_id': cur_lay_id}],
            'n_out': self.v_dim,
            'num_heads': self.num_heads,
            'total_key_dim': self.qk_dim}
        self.network[self.output_name]['unit']['dec_%(cur_lay_id)s_self_att_lin' % {'cur_lay_id': cur_lay_id}] = {
            'activation': None,
            'class': 'linear',
            'reuse_params': 'dec_0_self_att_lin' if self.tied_params else None,
            'forward_weights_init': self.forward_weights_initializer,
            'from': ['dec_%(cur_lay_id)s_self_att_att' % {'cur_lay_id': cur_lay_id}],
            'n_out': self.trans_out_dim,
            'with_bias': False}
        self.network[self.output_name]['unit']['dec_%(cur_lay_id)s_self_att_drop' % {'cur_lay_id': cur_lay_id}] = {
            'class': 'dropout', 'dropout': self.dropout,
            'from': ['dec_%(cur_lay_id)s_self_att_lin' % {'cur_lay_id': cur_lay_id}]}
        self.network[self.output_name]['unit']['dec_%(cur_lay_id)s_att_out' % {'cur_lay_id': cur_lay_id}] = {
            'class': 'combine',
            'from': ['dec_%(prev_lay_id)s' % {'prev_lay_id': prev_lay_id},
                     'dec_%(cur_lay_id)s_self_att_drop' % {'cur_lay_id': cur_lay_id}],
            'kind': 'add',
            'n_out': self.trans_out_dim,
            'trainable': True}
        self.network[self.output_name]['unit']['dec_%(cur_lay_id)s_ff_laynorm' % {'cur_lay_id': cur_lay_id}] = {
            'class': 'layer_norm', 'from': ['dec_%(cur_lay_id)s_att_out' % {'cur_lay_id': cur_lay_id}]}
        self.network[self.output_name]['unit']['dec_%(cur_lay_id)s_ff_conv1' % {'cur_lay_id': cur_lay_id}] = {
            'class': 'linear',
            'activation': self.act_func,
            'forward_weights_init': self.forward_weights_initializer,
            'reuse_params': 'dec_0_ff_conv1' if self.tied_params else None,
            'from': ['dec_%(cur_lay_id)s_ff_laynorm' % {'cur_lay_id': cur_lay_id}],
            'n_out': self.ff_dim,
            'with_bias': True}
        self.network[self.output_name]['unit']['dec_%(cur_lay_id)s_ff_conv2' % {'cur_lay_id': cur_lay_id}] = {
            'class': 'linear',
            'activation': None,
            'dropout': self.dropout,
            'reuse_params': 'dec_0_ff_conv2' if self.tied_params else None,
            'forward_weights_init': self.forward_weights_initializer,
            'from': ['dec_%(cur_lay_id)s_ff_conv1' % {'cur_lay_id': cur_lay_id}],
            'n_out': self.trans_out_dim,
            'with_bias': True}
        self.network[self.output_name]['unit']['dec_%(cur_lay_id)s_ff_drop' % {'cur_lay_id': cur_lay_id}] = {
            'class': 'dropout', 'dropout': self.dropout,
            'from': ['dec_%(cur_lay_id)s_ff_conv2' % {'cur_lay_id': cur_lay_id}]}
        self.network[self.output_name]['unit']['dec_%(cur_lay_id)s_ff_out' % {'cur_lay_id': cur_lay_id}] = {
            'class': 'combine', 'from': ['dec_%(cur_lay_id)s_att_out' % {'cur_lay_id': cur_lay_id},
                                         'dec_%(cur_lay_id)s_ff_drop' % {'cur_lay_id': cur_lay_id}],
            'kind': 'add', 'n_out': self.trans_out_dim}

    def construct(self):
        # Stack layers.
        cur_lay_id = 1
        prev_lay_id = 0
        for i in range(self.num_layers - 1):
            self._add_layer(cur_lay_id, prev_lay_id)
            cur_lay_id += 1
            prev_lay_id += 1

        # Add the final layer norm.
        self.network[self.output_name]['unit']['decoder'] = {'class': 'layer_norm', 'from': ['dec_%s' % prev_lay_id]}

        self.network[self.output_name]['unit']['output'] = {
            'class': 'softmax',
            'dropout': self.dropout,
            'forward_weights_init': self.forward_weights_initializer,
            'from': ['decoder'],
            'target': self.target,
            'n_out': vocabulary_size,
            'with_bias': True}
        if self.final_layer_update is not None:
            self.network[self.output_name]['unit']['output'].update(self.final_layer_update)

        return self.network


network = {}
student = Transformer(key='output',
                      num_heads=num_heads,
                      num_layers=num_layers,
                      ff_dim=ff_dim,
                      emb_dim=emb_dim,
                      qk_dim=qk_dim,
                      v_dim=v_dim,
                      trans_out_dim=trans_out_dim,
                      dropout=dropout,
                      att_dropout=dropout,
                      act_func=act_func,
                      weights_init=forward_weights_initializer,
                      target='data',
                      final_layer_update={
                        'use_transposed_weights': True,
                        'loss': 'sampling_loss',
                        'loss_opts': {
                            'num_sampled': num_sampled,
                            'sampler': 'log_uniform',
                            'use_full_softmax': full_softmax,
                            'nce_log_norm_term': 0.0,
                            'nce_loss': use_nce},
                        'bias_init': b_init},
                      trainable=True)
network.update(student.construct())


# ==========================================================
# DETAILS
# ==========================================================

num_inputs = vocabulary_size

extern_data = {"data": {"dim": num_inputs, "sparse": True, "dtype": "int32"}}  # sparse data
extern_data["delayed"] = extern_data["data"]
target = "data"

_cf_cache = {}


def cf(filename):
    """Cache manager"""
    if not use_cache_manager:
        return filename
    if filename in _cf_cache:
        return _cf_cache[filename]
    if check_output(["hostname"]).strip() in ["cluster-cn-211", "sulfid"]:
        print("use local file: %s" % filename)
        return filename  # for debugging
    cached_fn = check_output(["cf", filename]).strip().decode("utf8")
    assert os.path.exists(cached_fn)
    _cf_cache[filename] = cached_fn
    return cached_fn


def get_dataset(key, **kwargs):
    assert key in ['train', 'valid', 'test']

    up = {
        'class': 'LmDataset',
        'corpus_file': lambda: list(map(cf, data_files[key])),
        'orth_symbols_map_file': lambda: cf(vocab_file),
        'word_based': True,
        'seq_end_symbol': seq_boundary,
        'auto_replace_unknown_symbol': replace_unk_auto,
        'unknown_symbol': unk_token,
        'add_delayed_seq_data': True,
        'delayed_seq_data_start_symbol': seq_boundary,
        'seq_ordering': 'sorted',
        'partition_epoch': 1,
        'parse_orth_opts': {
            'square_brackets_for_specials': False}}
    up.update(kwargs)

    return up


train = get_dataset("train",
                    partition_epoch=train_partition,
                    seq_ordering=seq_order)
dev = get_dataset("valid")
eval = get_dataset('test') if do_test_dataset else None

