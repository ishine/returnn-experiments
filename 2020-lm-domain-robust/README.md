Examples LSTM and Transformer language model training configs for knowledge distillation using
sampled softmax or NCE loss.

Please cite this paper when making use of these configs:

```
@InProceedings{gerstenberger:icassp20,
author= {Gerstenberger, Alexander and Irie, Kazuki and Golik, Pavel and Beck, Eugen and Ney, Hermann},
title= {Domain Robust, Fast, and Compact Neural Language Models},
booktitle= {IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
year= 2020,
address= {Barcelona, Spain},
month= may,
}
```
