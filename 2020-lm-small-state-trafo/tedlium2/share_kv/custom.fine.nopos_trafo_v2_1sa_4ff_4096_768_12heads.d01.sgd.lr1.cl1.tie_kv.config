#!python3

# TED-LIUM 2, small state 8L-3F Transformer LM with shared KV
# Table 4 in the paper.

# This config is for fine-tuning.
# The config for the pre-training is the same but with learning rate 1.,
# and the line below commented out.
import_model_train_epoch1 = "/work/asr4/irie/experiments/lm/tedlium2/2019-07-28--word-152k/data-train/nopos_trafo_v2_1sa_4ff_4096_768_12heads.d01.sgd.lr1.cl1.tie_kv/net-model/network.035"

import os
from subprocess import check_output
from Util import cleanup_env_var_path
cleanup_env_var_path("LD_LIBRARY_PATH", "/u/zeyer/tools/glibc217")

# task
use_tensorflow = True
task = "train"
device = "gpu"
multiprocessing = True
update_on_device = True

_cf_cache = {}

def cf(filename):
    """Cache manager"""
    if filename in _cf_cache:
        return _cf_cache[filename]
    if check_output(["hostname"]).strip() in ["cluster-cn-211", "sulfid"]:
        print("use local file: %s" % filename)
        return filename  # for debugging
    cached_fn = check_output(["cf", filename]).strip().decode("utf8")
    assert os.path.exists(cached_fn)
    _cf_cache[filename] = cached_fn
    return cached_fn

data_files = {
    "train": ["/work/asr3/irie/data/tedlium2/word_150k/data/train.en.gz",
              "/work/asr3/irie/data/tedlium2/word_150k/data/commoncrawl-9pc.en.gz"],
    "cv": ["/work/asr3/irie/data/tedlium2/word_150k/data/dev.en.gz"],
    "test": ["/work/asr3/irie/data/tedlium2/word_150k/data/eval.en.gz"]}
vocab_file = "/work/asr3/irie/data/tedlium2/word_150k/data/vocab.returnn.txt"

orth_replace_map_file = ""

num_inputs = 152267

train_epoch_split = 10

epoch_split = {"train": train_epoch_split, "cv": 1, "test": 1}
seq_order = {
    "train": "random",
    "cv": "sorted",
    "test": "sorted"}

def get_dataset(data):
    assert data in ["train", "cv", "test"]
    return {
        "class": "LmDataset",
        "corpus_file": lambda: list(map(cf, data_files[data])),
        "orth_symbols_map_file": lambda: cf(vocab_file),
        "orth_replace_map_file": orth_replace_map_file,
        "word_based": True,
        "seq_end_symbol": "<sb>",
        "auto_replace_unknown_symbol": True,
        "unknown_symbol": "<unk>",
        "add_delayed_seq_data": True,
        "delayed_seq_data_start_symbol": "<sb>",
        "seq_ordering": seq_order[data],
        "partition_epoch": epoch_split[data]
    }

train = get_dataset("train")
dev = get_dataset("cv")
eval = get_dataset("test")
cache_size = "0"
window = 1

# network
# (also defined by num_inputs & num_outputs)
num_outputs = {"data": {"dim": num_inputs, "sparse": True, "dtype": "int32"}}  # sparse data
num_outputs["delayed"] = num_outputs["data"]
target = "data"

tf_session_opts = {'allow_soft_placement': True, 'log_device_placement': False}

# Transformer params.
num_layers = 8 
num_ff_per_block = 3
ff_dim = 4096
num_heads = 12
emb_dim = 128
qk_dim = 768
v_dim = qk_dim
trans_out_dim = qk_dim
dropout = 0.1
att_dropout = 0.1
act_func = "relu"

# Universal.
tied_params = False

# Output layer.
output_sampling_loss = False
output_num_sampled = 4096
output_use_full_softmax = False

# Input embedding.
place_emb_on_cpu = False  # E.g. for adagrad.

# Initializer.
forward_weights_initializer = "variance_scaling_initializer(mode='fan_in', distribution='uniform', scale=1.0)"

network = {
'output': {'class': 'rec',
  'from': ['data:delayed'],
  'target': 'data',
  'trainable': True,
  'unit': {'target_embed_raw': { 'activation': None,
                                 "param_device": "CPU" if place_emb_on_cpu else None,
                                 'class': 'linear',
                                 'forward_weights_init': forward_weights_initializer,
                                 'from': ['data:source'],
                                 'n_out': emb_dim,
                                 'with_bias': False},
           'target_embed': {'class': 'dropout', 'dropout': dropout, 'from': ['target_embed_raw']},
           'target_embed_lin': { 'activation': None,
                                  'class': 'linear',
                                  'forward_weights_init': forward_weights_initializer,
                                  'from': ['target_embed'],
                                  'n_out': trans_out_dim,
                                  'with_bias': False},
           'dec_0': {'class': 'copy', 'from': ['dec_0_ff_out']},
           'dec_0_self_att_laynorm': {'class': 'layer_norm', 'from': ['target_embed_lin']},
           'dec_0_self_att_att': { 'attention_dropout': dropout,
                                    'attention_left_only': True,
                                    'class': 'self_attention_share_kv',
                                    'share_kv': True,
                                    'forward_weights_init': forward_weights_initializer,
                                    'from': ['dec_0_self_att_laynorm'],
                                    'n_out': v_dim,
                                    'num_heads': num_heads,
                                    'total_key_dim': qk_dim},
           'dec_0_self_att_lin': { 'activation': None,
                                    'class': 'linear',
                                    'forward_weights_init': forward_weights_initializer,
                                    'from': ['dec_0_self_att_att'],
                                    'n_out': trans_out_dim,
                                    'with_bias': False},
           'dec_0_self_att_drop': {'class': 'dropout', 'dropout': dropout, 'from': ['dec_0_self_att_lin']},
           'dec_0_att_out': { 'class': 'combine',
                               'from': ['target_embed_lin', 'dec_0_self_att_drop'],
                               'kind': 'add',
                               'n_out': trans_out_dim,
                               'trainable': True},
           'dec_0_ff_laynorm': {'class': 'layer_norm', 'from': ['dec_0_att_out']},
           'dec_0_ff_conv1': { 'activation': act_func,
                                'class': 'linear',
                                'forward_weights_init': forward_weights_initializer,
                                'from': ['dec_0_ff_laynorm'],
                                'n_out': ff_dim,
                                'with_bias': True},
           'dec_0_ff_conv2': { 'activation': None,
                                'class': 'linear',
                                'dropout': dropout,
                                'forward_weights_init': forward_weights_initializer,
                                'from': ['dec_0_ff_conv1'],
                                'n_out': trans_out_dim,
                                'with_bias': True},
           'dec_0_ff_drop': {'class': 'dropout', 'dropout': dropout, 'from': ['dec_0_ff_conv2']},
           'dec_0_ff_out': {'class': 'combine', 'from': ['dec_0_att_out', 'dec_0_ff_drop'], 'kind': 'add', 'n_out': trans_out_dim},}}}


def add_trafo_layer(cur_lay_id, prev_lay_id, name_input):
  network['output']['unit']['dec_%(cur_lay_id)s' % {'cur_lay_id': cur_lay_id} ] = {
    'class': 'copy', 'from': ['dec_%(cur_lay_id)s_ff_out' % {'cur_lay_id': cur_lay_id} ]}  # Just renaming the block output to dec_N.

  network['output']['unit']['dec_%(cur_lay_id)s_self_att_laynorm' % {'cur_lay_id': cur_lay_id} ] = {
    'class': 'layer_norm', 'from': [name_input]}  # Actual input layer of the block.

  network['output']['unit']['dec_%(cur_lay_id)s_self_att_att' % {'cur_lay_id': cur_lay_id} ] = {
    'attention_dropout': dropout,
    'attention_left_only': True,
    'reuse_params': 'dec_0_self_att_att' if tied_params else None,
    'class': 'self_attention_share_kv',
    'share_kv': True,
    'forward_weights_init': forward_weights_initializer,
    'from': ['dec_%(cur_lay_id)s_self_att_laynorm' % {'cur_lay_id': cur_lay_id}],
    'n_out': v_dim,
    'num_heads': num_heads,
    'total_key_dim': qk_dim}
  network['output']['unit']['dec_%(cur_lay_id)s_self_att_lin' % {'cur_lay_id': cur_lay_id} ] = {
    'activation': None,
    'class': 'linear',
    'reuse_params': 'dec_0_self_att_lin' if tied_params else None,
    'forward_weights_init': forward_weights_initializer,
    'from': ['dec_%(cur_lay_id)s_self_att_att' % {'cur_lay_id': cur_lay_id}],
    'n_out': trans_out_dim,
    'with_bias': False}
  network['output']['unit']['dec_%(cur_lay_id)s_self_att_drop' % {'cur_lay_id': cur_lay_id} ] = {
    'class': 'dropout', 'dropout': dropout, 'from': ['dec_%(cur_lay_id)s_self_att_lin' % {'cur_lay_id': cur_lay_id}]}
  network['output']['unit']['dec_%(cur_lay_id)s_att_out' % {'cur_lay_id': cur_lay_id} ] = {
    'class': 'combine',
    'from': [name_input, 'dec_%(cur_lay_id)s_self_att_drop' % {'cur_lay_id': cur_lay_id}],
    'kind': 'add',
    'n_out': trans_out_dim,
    'trainable': True}
  network['output']['unit']['dec_%(cur_lay_id)s_ff_laynorm' % {'cur_lay_id': cur_lay_id}] = {
    'class': 'layer_norm', 'from': ['dec_%(cur_lay_id)s_att_out' % {'cur_lay_id': cur_lay_id}]}
  network['output']['unit']['dec_%(cur_lay_id)s_ff_conv1' % {'cur_lay_id': cur_lay_id}] = {
                       'class': 'linear',
                       'activation': act_func,
                       'forward_weights_init': forward_weights_initializer,
                       'reuse_params': 'dec_0_ff_conv1' if tied_params else None,
                       'from': ['dec_%(cur_lay_id)s_ff_laynorm' % {'cur_lay_id': cur_lay_id}],
                       'n_out': ff_dim,
                       'with_bias': True}
  network['output']['unit']['dec_%(cur_lay_id)s_ff_conv2' % {'cur_lay_id': cur_lay_id} ] = {
                       'class': 'linear',
                       'activation': None,
                       'dropout': dropout,
                       'reuse_params': 'dec_0_ff_conv2' if tied_params else None,
                       'forward_weights_init': forward_weights_initializer,
                       'from': ['dec_%(cur_lay_id)s_ff_conv1' % {'cur_lay_id': cur_lay_id}],
                       'n_out': trans_out_dim,
                       'with_bias': True}
  network['output']['unit']['dec_%(cur_lay_id)s_ff_drop' % {'cur_lay_id': cur_lay_id}] = {
    'class': 'dropout', 'dropout': dropout, 'from': ['dec_%(cur_lay_id)s_ff_conv2' % {'cur_lay_id': cur_lay_id}]}
  network['output']['unit']['dec_%(cur_lay_id)s_ff_out' % {'cur_lay_id': cur_lay_id}] = {
    'class': 'combine', 'from': ['dec_%(cur_lay_id)s_att_out' % {'cur_lay_id': cur_lay_id}, 'dec_%(cur_lay_id)s_ff_drop' % {'cur_lay_id': cur_lay_id}],
    'kind': 'add', 'n_out': trans_out_dim}

  return 'dec_%(cur_lay_id)s' % {'cur_lay_id': cur_lay_id}



def add_dnn_layer(cur_lay_id, prev_lay_id, dnn_id, name_input):
  network['output']['unit']['dec_%(cur_lay_id)s_dnn_%(dnn_id)s' % {'cur_lay_id': cur_lay_id, 'dnn_id': dnn_id} ] = {
    'class': 'copy', 'from': ['dec_%(cur_lay_id)s_dnn_%(dnn_id)s_ff_out' % {'cur_lay_id': cur_lay_id, 'dnn_id': dnn_id} ]}  # Just renaming the block output to dec_N.

  network['output']['unit']['dec_%(cur_lay_id)s_dnn_%(dnn_id)s_ff_laynorm' % {'cur_lay_id': cur_lay_id, 'dnn_id': dnn_id}] = {
    'class': 'layer_norm', 'from': [name_input]}

  network['output']['unit']['dec_%(cur_lay_id)s_dnn_%(dnn_id)s_ff_conv1' % {'cur_lay_id': cur_lay_id, 'dnn_id': dnn_id}] = {
                       'class': 'linear',
                       'activation': 'relu',
                       'forward_weights_init': forward_weights_initializer,
                       'reuse_params': 'dec_0_ff_conv1' if tied_params else None,
                       'from': ['dec_%(cur_lay_id)s_dnn_%(dnn_id)s_ff_laynorm' % {'cur_lay_id': cur_lay_id, 'dnn_id': dnn_id}],
                       'n_out': ff_dim,
                       'with_bias': True}

  network['output']['unit']['dec_%(cur_lay_id)s_dnn_%(dnn_id)s_ff_conv2' % {'cur_lay_id': cur_lay_id, 'dnn_id': dnn_id} ] = {
                       'class': 'linear',
                       'activation': None,
                       'dropout': dropout,
                       'reuse_params': 'dec_0_ff_conv2' if tied_params else None,
                       'forward_weights_init': forward_weights_initializer,
                       'from': ['dec_%(cur_lay_id)s_dnn_%(dnn_id)s_ff_conv1' % {'cur_lay_id': cur_lay_id, 'dnn_id': dnn_id}],
                       'n_out': trans_out_dim,
                       'with_bias': True}

  network['output']['unit']['dec_%(cur_lay_id)s_dnn_%(dnn_id)s_ff_drop' % {'cur_lay_id': cur_lay_id, 'dnn_id': dnn_id}] = {
    'class': 'dropout', 'dropout': dropout, 'from': ['dec_%(cur_lay_id)s_dnn_%(dnn_id)s_ff_conv2' % {'cur_lay_id': cur_lay_id, 'dnn_id': dnn_id}]}

  network['output']['unit']['dec_%(cur_lay_id)s_dnn_%(dnn_id)s_ff_out' % {'cur_lay_id': cur_lay_id, 'dnn_id': dnn_id}] = {
    'class': 'combine', 'from': [name_input, 'dec_%(cur_lay_id)s_dnn_%(dnn_id)s_ff_drop' % {'cur_lay_id': cur_lay_id, 'dnn_id': dnn_id}],
    'kind': 'add', 'n_out': trans_out_dim}

  return 'dec_%(cur_lay_id)s_dnn_%(dnn_id)s' % {'cur_lay_id': cur_lay_id, 'dnn_id': dnn_id}



# Stack layers.
cur_lay_id = 1
prev_lay_id = 0
name_input = "dec_0"
for i in range(num_layers-1):
  name_input = add_trafo_layer(cur_lay_id, prev_lay_id, name_input)
  for j in range(num_ff_per_block):
    name_input = add_dnn_layer(cur_lay_id, prev_lay_id, j, name_input)
  cur_lay_id += 1
  prev_lay_id += 1

# Add the final layer norm.
network['output']['unit']['decoder'] = {'class': 'layer_norm', 'from': [name_input]}

# Add output layer and loss.
if output_sampling_loss:
  network['output']['unit']['output'] = {
    'class': 'softmax', 'dropout': dropout, 'use_transposed_weights': True,
    'loss_opts': {'num_sampled': output_num_sampled, 'use_full_softmax': output_use_full_softmax, 'nce_loss': False},
    'forward_weights_init': forward_weights_initializer,
    'loss': 'sampling_loss', 'target': 'data', 'from': ['decoder']}
else:
  network['output']['unit']['output'] = {
    'class': 'softmax',
    'dropout': dropout,
    'forward_weights_init': forward_weights_initializer,
    'from': ['decoder'],
    'loss': 'ce',
    'target': 'data',
    'with_bias': True}

############
# Training hyper-params.
batching = "random"
batch_size = 900
max_seq_length = 602
max_seqs = 64
chunking = "0"
num_epochs = 60
gradient_clip_global_norm = 1.
gradient_noise = 0.
learning_rate = 0.25
learning_rate_control = "newbob_rel"
learning_rate_control_relative_error_relative_lr = False
newbob_multi_num_epochs = train_epoch_split
newbob_relative_error_div_by_old = True

newbob_learning_rate_decay = 0.95
newbob_relative_error_threshold = -0.007
newbob_multi_update_interval = 1
learning_rate_control_error_measure = "dev_score_output:exp"

learning_rate_file = "newbob.data"
model = "net-model/network"

calculate_exp_loss = True
cleanup_old_models = {"keep_best_n": 1, "keep_last_n": 2}

# log
log = "log/crnn.%s.log" % task
log_verbosity = 4

####################################################################
# Self attention layer with an option to share key and value.
# This change has not been pushed to overwrite the self-attention layer
# as the modification is too specific.
# See discussion at https://github.com/rwth-i6/returnn/pull/239

from TFNetworkLayer import register_layer_class, _ConcatInputLayer

class SelfAttentionLayerShareKV(_ConcatInputLayer):
  """
  Applies self-attention on the input. I.e., with input `x`,
  it will basically calculate

      att(Q x, K x, V x),

  where `att` is multi-head dot-attention for now, `Q`, `K`, `V` are matrices.
  The attention will be over the time-dimension.
  If there is no time-dimension, we expect to be inside a :class:`RecLayer`;
  also, this is only valid with `attention_to_past_only=True`.

  See also `dot_product_attention` here:
    https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py
  """
  layer_class = "self_attention_share_kv"
  recurrent = True

  def __init__(self, num_heads, total_key_dim,
               key_shift=None, share_kv=False,
               forward_weights_init="glorot_uniform", attention_dropout=0.0,
               attention_left_only=False, initial_state=None, restrict_state_to_last_seq=False, **kwargs):
    """
    :param int num_heads:
    :param int total_key_dim: i.e. key_dim == total_key_dim // num_heads
    :param LayerBase|None key_shift: additive term to the key. can be used for relative positional encoding.
      Should be of shape (num_queries,num_keys,key_dim), currently without batch-dimension.
      I.e. that should be shape (1,t,key_dim) inside rec-layer or (T,T,key_dim) outside.
    :param bool share_kv: if True, use the same parameters for key and value. total_key_dim and n_out must match.
    :param str forward_weights_init: see :func:`TFUtil.get_initializer`
    :param float attention_dropout:
    :param bool attention_left_only: will mask out the future. see Attention is all you need.
    :param str|float|int|None initial_state: see RnnCellLayer.get_rec_initial_state_inner().
    :param bool restrict_state_to_last_seq: see code comment below
    """
    super(SelfAttentionLayerShareKV, self).__init__(**kwargs)
    self._restrict_state_to_last_seq = restrict_state_to_last_seq
    assert self._rec_previous_layer or self.input_data.time_dim_axis is not None, (
      "%s: This layer is expected to be used inside a RecLayer, or to have input with time." % self)
    total_value_dim = self.output.dim
    assert total_key_dim % num_heads == 0, "must be divisible"
    assert total_value_dim % num_heads == 0, "must be divisible. total_value_dim = n_out"
    kv_dim = total_key_dim + total_value_dim  # size of self-attention state per time
    if share_kv:
      assert total_key_dim == total_value_dim, "must be the same dimension"
      kv_dim = total_key_dim
    from TFUtil import get_initializer, dot, get_shape, to_int32_64
    with self.var_creation_scope():
      fwd_weights_initializer = get_initializer(
        forward_weights_init, seed=self.network.random.randint(2 ** 31), eval_local_ns={"layer": self})
      n_in = self.input_data.dim
      qkv_dims = [total_key_dim, total_key_dim, total_value_dim]  # Q, K, V
      if share_kv:
        del qkv_dims[0]
      mat_n_out = sum(qkv_dims)
      import tensorflow as tf
      mat = self.add_param(tf.get_variable(
        name="QKV", shape=(n_in, mat_n_out), dtype=tf.float32, initializer=fwd_weights_initializer),
        axes_split_info=[[n_in], qkv_dims])
      if self._rec_previous_layer:
        assert self.input_data.time_dim_axis is None
        assert attention_left_only
        # (batch,heads,time,kv-dim//heads)
        prev_kv_left = self._rec_previous_layer.rec_vars_outputs["kv_left"]
      else:
        assert self.input_data.time_dim_axis is not None
        batch_dim = self.input_data.get_batch_dim()
        prev_kv_left = (
          RnnCellLayer.get_rec_initial_state_inner(
            initial_state=initial_state, name=self.name, rec_layer=self,
            state_key="kv_left",
            initial_shape=(batch_dim, num_heads, 0, kv_dim // num_heads),
            shape_invariant=(None, num_heads, None, kv_dim // num_heads))
          if initial_state is not None else None)
    x = self.input_data.placeholder
    if self.input_data.sparse:
      x = tf.nn.embedding_lookup(mat, to_int32_64(x))
    else:
      x = dot(x, mat)
    x.set_shape(tf.TensorShape(self.input_data.batch_shape_dense[:-1] + (mat_n_out,)))
    x_shape = [-1, -1, num_heads, mat_n_out // num_heads]  # without time
    if self.input_data.time_dim_axis is None:
      assert self.input_data.batch_dim_axis == 0
      x_shape[1] = 1
    else:
      assert self.input_data.time_dim_axis in (0, 1)
    assert self.input_data.batch_dim_axis in (0, 1)
    batch_dim = tf.shape(x)[self.input_data.batch_dim_axis]
    x_shape[self.input_data.batch_dim_axis] = batch_dim
    x = tf.reshape(x, x_shape)  # (batch,time|1)|(time|1,batch) + (heads,qkv-dim//heads)
    x.set_shape(tf.TensorShape([None, None, num_heads, mat_n_out // num_heads]))
    assert self.input_data.batch_dim_axis in (0, 1)
    # (batch,heads,time|1,qkv-dim//heads)
    x = tf.transpose(x, [self.input_data.batch_dim_axis, 2, 1 - self.input_data.batch_dim_axis, 3])
    x.set_shape((None, num_heads, None, mat_n_out // num_heads))
    qkv_head_dims = [total_key_dim // num_heads, total_key_dim // num_heads, total_value_dim // num_heads]
    if share_kv:
      del qkv_head_dims[0]
      q, k = tf.split(x, qkv_head_dims, axis=-1, name="qkv")
      v = k
    else:
      q, k, v = tf.split(x, qkv_head_dims, axis=-1, name="qkv")
    q.set_shape((None, num_heads, None, total_key_dim // num_heads))
    k.set_shape((None, num_heads, None, total_key_dim // num_heads))
    v.set_shape((None, num_heads, None, total_value_dim // num_heads))
    q *= (total_key_dim // num_heads) ** -0.5
    if prev_kv_left is not None:
      if share_kv:  # Memory for k.
        self.rec_vars_outputs["kv_left"] = k  # usually will be overwritten by the new kv below
        kv = tf.concat([prev_kv_left, k], axis=2)
        kv.set_shape((None, num_heads, None, total_key_dim // num_heads))
      else:  # Memory for kv.
        kv = tf.concat([k, v], axis=-1)  # (batch,heads,1|time,kv-dim//heads)
        kv.set_shape((None, num_heads, None, (total_key_dim + total_value_dim) // num_heads))
        self.rec_vars_outputs["kv_left"] = kv  # usually will be overwritten by the new kv below
        kv = tf.concat([prev_kv_left, kv], axis=2)
        kv.set_shape((None, num_heads, None, (total_key_dim + total_value_dim) // num_heads))
      if restrict_state_to_last_seq:
        # 'Last' means the current `kv` here, before the concat with `prev_kv_left`.
        # I.e. we wont update `rec_vars_outputs` to the concatenated variant; it will exclude `prev_kv_left`.
        # Note that this means a difference depending whether we are inside the loop or not.
        # If we are inside the loop, we should update until the end of the seq, and then restrict to the last seq.
        # This is handled in post_process_final_rec_vars_outputs.
        # Otherwise just leave `rec_vars_outputs` as it is already.
        if self._rec_previous_layer:
          self.rec_vars_outputs["kv_left"] = kv
      else:  # this is usually the case
        self.rec_vars_outputs["kv_left"] = kv
      if share_kv:
        k, v = kv, kv
      else:
        k, v = tf.split(kv, [total_key_dim // num_heads, total_value_dim // num_heads], axis=-1)
    # Dot-attention. Resulting last time dimension will be used to perform the softmax over, and will the be reduced.
    energy = tf.matmul(q, k, transpose_b=True)  # (batch,heads,num_queries|1,num_keys), e.g. (batch,heads,time|1,time)
    if key_shift:
      # We could add it to `k`, but instead, to avoid unbroadcasting, we do it as an additional matmul.
      # key_shift expected to be of shape (num_queries|1,num_keys,key_dim).
      key_shift_data = key_shift.output
      assert key_shift_data.batch_dim_axis is None and key_shift_data.dim == total_key_dim // num_heads
      k_ = key_shift_data.placeholder
      # See also _relative_attention_inner here: https://github.com/tensorflow/tensor2tensor
      q_t = tf.transpose(q, [2, 0, 1, 3])  # [num_queries|1,batch,heads,key_dim]
      q_t_r = tf.reshape(
        q_t, [tf.shape(q_t)[0], batch_dim * num_heads, total_key_dim // num_heads])  # [num_queries|1,batch*heads,k-dim]
      with tf.control_dependencies([tf.assert_equal(
            message="check_shape_of_key_shift:",
            x=tf.shape(k_), y=[tf.shape(q_t)[0], tf.shape(energy)[-1], total_key_dim // num_heads])]):
        energy_ = tf.matmul(q_t_r, k_, transpose_b=True)  # [num_queries|1,batch*heads,num_keys]
      energy_ = tf.reshape(
        energy_, [tf.shape(q_t)[0], batch_dim, num_heads, tf.shape(energy)[-1]])  # [num_queries|1,batch,heads,num_keys]
      energy_ = tf.transpose(energy_, [1, 2, 0, 3])  # [batch,heads,num_queries|1,num_keys]
      energy += energy_
    if self.input_data.time_dim_axis is not None:
      if attention_left_only:
        # We also ignore the input data sequence length, because we expect that frames outside the seq length
        # are anyway ignored.
        from TFUtil import matrix_triangular
        num_queries = tf.shape(energy)[2]
        num_keys = tf.shape(energy)[-1]
        # (1,1,num_queries,num_keys)
        energy_mask = matrix_triangular((1, 1, num_queries, num_keys), dtype=tf.bool, lower=True)
      else:
        energy_mask = tf.sequence_mask(
          self.input_data.get_sequence_lengths(), maxlen=tf.shape(energy)[-1])  # (batch,time)
        energy_mask = tf.reshape(energy_mask, [tf.shape(energy)[0], 1, 1, tf.shape(energy)[-1]])  # (batch,1,1,time)
      # Currently tf.where does not support broadcasting...
      energy_mask = tf.logical_and(energy_mask, tf.ones_like(energy, dtype=tf.bool))
      energy = tf.where(energy_mask, energy, float("-inf") * tf.ones_like(energy), name="energy_masked")
    weights = tf.nn.softmax(energy)  # (batch,heads,time,time)
    if attention_dropout:
      import TFUtil
      weights = self.network.cond_on_train(
        fn_train=lambda: TFUtil.dropout(
          weights,
          keep_prob=1 - attention_dropout,
          seed=self.network.random.randint(2 ** 31)),
        fn_eval=lambda: weights)
    v = tf.matmul(weights, v, name="reduce_att")  # (batch,heads,time,v-dim//heads)
    v.set_shape(tf.TensorShape([None, num_heads, None, total_value_dim // num_heads]))
    v = tf.transpose(v, [0, 2, 1, 3])  # (batch,time,heads,v-dim//heads)
    v = tf.reshape(v, get_shape(v)[:2] + [total_value_dim], name="merge_vdim")  # (batch,time,v-dim)
    v.set_shape(tf.TensorShape([None, None, total_value_dim]))

    if self.input_data.time_dim_axis is None:
      # Squeeze away the time-dim, which should be 1.
      v = tf.squeeze(v, axis=1)
    self.output.placeholder = v
    self.output.size_placeholder = self.input_data.size_placeholder.copy()

  @classmethod
  def transform_config_dict(cls, d, network, get_layer):
    """
    :param dict[str] d:
    :param TFNetwork.TFNetwork network:
    :param get_layer:
    """
    assert isinstance(d, dict)
    super(SelfAttentionLayerShareKV, cls).transform_config_dict(d, network=network, get_layer=get_layer)
    if d.get("key_shift", None):
      d["key_shift"] = get_layer(d["key_shift"])

  @classmethod
  def get_out_data_from_opts(cls, n_out, name, sources, **kwargs):
    """
    :param int n_out:
    :param str name:
    :param list[LayerBase] sources:
    :rtype: Data
    """
    assert sources
    defined_sources = [src for src in sources if src and not src.output.undefined]
    if not defined_sources:
      from TFNetwork import CannotHandleUndefinedSourcesException
      raise CannotHandleUndefinedSourcesException(
        layer_name=name, layer_desc=dict(n_out=n_out, sources=sources, **kwargs))
    import numpy
    out = defined_sources[0].output.copy_as_batch_major().copy(name="%s_output" % name)
    if out.sparse:
      out.dtype = "float32"
      out.sparse = False
      out.shape = out.shape + (out.dim,)
    out.dim = n_out
    if len(out.shape) >= 2:
      if all(out.shape[:-1]):
        out.shape = (numpy.prod(out.shape[:-1]), n_out)
      else:
        out.shape = (None, n_out)
    else:
      out.shape = (n_out,)
    return out

  # noinspection PyMethodOverriding
  @classmethod
  def get_rec_initial_extra_outputs(cls, batch_dim, rec_layer, num_heads, total_key_dim, n_out, name,
                                    initial_state=None, sources=(), **kwargs):
    """
    :param tf.Tensor batch_dim:
    :param RecLayer|LayerBase rec_layer:
    :param int num_heads:
    :param int total_key_dim:
    :param int n_out:
    :param str name:
    :param str|float|int|None initial_state:
    :param list[LayerBase] sources:
    :rtype: dict[str, tf.Tensor]
    """
    data = get_concat_sources_data_template(sources)
    data = data.copy_as_batch_major()
    if data.time_dim_axis is None or initial_state is not None:
      kv_dim = total_key_dim + n_out
      # Assume inside RecLayer, or initial_state set explicitly.
      # Before, we used a tf.TensorArray.
      # However, that has higher memory consumptions than just using a tensor and concatenating to it.
      # (batch,heads,time,kv-dim//heads)
      kv_left = RnnCellLayer.get_rec_initial_state_inner(
        rec_layer=rec_layer, state_key="kv_left",
        name=name, initial_state=initial_state,
        initial_shape=(batch_dim, num_heads, 0, kv_dim // num_heads),
        shape_invariant=(None, num_heads, None, kv_dim // num_heads))
      return {"kv_left": kv_left}
    return {}

  @classmethod
  def get_rec_initial_extra_outputs_shape_invariants(cls, num_heads, total_key_dim, n_out, sources, **kwargs):
    """
    :param int num_heads:
    :param int total_key_dim:
    :param int n_out:
    :param list[LayerBase] sources:
    :rtype: dict[str, tf.TensorShape]
    """
    data = get_concat_sources_data_template(sources)
    data = data.copy_as_batch_major()
    import tensorflow as tf
    if data.time_dim_axis is None:
      # Assume inside RecLayer. See get_rec_initial_extra_outputs.
      total_value_dim = n_out
      return {"kv_left": tf.TensorShape((None, num_heads, None, (total_key_dim + total_value_dim) // num_heads))}
    return {}

  def post_process_final_rec_vars_outputs(self, rec_vars_outputs, seq_len):
    """
    :param dict[str,tf.Tensor] rec_vars_outputs:
    :param tf.Tensor seq_len: shape (batch,)
    :rtype: dict[str,tf.Tensor]
    """
    import tensorflow as tf
    if self.input_data.time_dim_axis is None and self._restrict_state_to_last_seq:
      # kv_left should be of shape (batch, heads, time, kv_dim_per_head).
      # time will be >= max(seq_len); could be more if we use e.g. initial_state=keep_over_epoch.
      rec_vars_outputs["kv_left"] = rec_vars_outputs["kv_left"][:, :, -tf.reduce_max(seq_len):]
    return rec_vars_outputs

register_layer_class(SelfAttentionLayerShareKV)
