Small state Transformer language model training example configs, from the paper:

How Much Self-Attention Do We Need? Trading Attention for Feed-Forward Layers.

Please cite this paper when making use of these configs:

```
@InProceedings{irie:icassp20,
author= {Irie, Kazuki and Gerstenberger, Alexander and Schl√ºter, Ralf and Ney, Hermann},
title= {How Much Self-Attention Do We Need? Trading Attention for Feed-Forward Layers},
booktitle= {IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
year= 2020,
address= {Barcelona, Spain},
month= may,
}
```

**Pre-trained models** are available for download: [TED-LIUM 2](http://www-i6.informatik.rwth-aachen.de/~irie/models/tedlium2/2020-lm-small-state-trafo) and [LibriSpeech](http://www-i6.informatik.rwth-aachen.de/~irie/models/librispeech/2020-lm-small-state-trafo).
